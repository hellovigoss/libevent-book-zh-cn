include::license.txt[]

:language: C

对于异步IO 的简单介绍
----------------------------------------

大多数初阶程序员最先接触的都是阻塞IO调用。
当你进行IO调用时，
系统并不会立刻返回直到它的业务逻辑处理完毕
，或者直到网络超时，那么，这个IO请求就是同步的。  当你在一个TCP连接上调用"connect()"
，相当于，你的操作系统以队列的方式发送了一个SYN包至
TCP连接的另一端主机。  在收到对方主机回应的SYN ACK包之前，它并不会
把控制权交还给你的应用，
亦或是系统超时
导致放弃挣扎。

这里有一个简单的阻塞式网络客户端的
调用示例。  它连接至www.google.com，发送一个简单的
HTTP请求，并且将响应结果输出值stdout。

//BUILD: SKIP
.Example: A simple blocking HTTP client
[code,C]
-------
include::examples_01/01_sync_webclient.c[]
-------

以上的所有网络请求都是阻塞式的：
gethostbyname只有在查询到www.google.com成功或者失败解析后
才会真正返回；connect不会return除非真正的
建立了连接；recv调用不会返回除非接收到了数据
或者被服务端关闭；send请求不会返回除非
kernel的写缓冲区被flush。

当然，阻塞式IO也并非完全是垃圾。  如果不需要
你的程序在处理网络请求间歇做一些别的工作，阻塞式的IO不会出啥篓子
。  但是一旦你希望程序同时
处理多个连接。  比如这个场景：
假设你需要从两个连接中读取数据，并且你不确定
哪一个连接会先得到输入。  你可不能这样

//BUILD: FUNCTIONBODY INC:../example_stubs/sec01.h
.Bad Example
[code,C]
-------
/* This won't work. */
char buf[1024];
int i, n;
while (i_still_want_to_read()) {
    for (i=0; i<n_sockets; ++i) {
        n = recv(fd[i], buf, sizeof(buf), 0);
        if (n==0)
            handle_close(fd[i]);
        else if (n<0)
            handle_error(fd[i], errno);
        else
            handle_input(fd[i], buf, n);
    }
}
-------
因为一旦fd[2]的数据先来，你的程序就emo了
它在读到fd[0]和fd[1]的数据之前动都不会动一下
。

使用多线程
或者多进程服务器是一种解决方案。  并行处理的一种简单方案是
使用一个单独的进程(或线程)来处理每个连接。
一旦每个连接有一个单独的进程进行处理，一个进程内的阻塞式IO调用
就不会影响到其他导致block。

以下是另一个程序示例。  演示了一个小型的服务器，
监听40713端口的TCP连接，从建立连接的输入中
一次读取一行数据
。  通过Unix 系统调用fork() 来创建一个新的进程来
处理所有请求建立的连接。

//BUILD: SKIP
.Example: Forking ROT13 server
[code,C]
-------
include::examples_01/01_rot13_server_forking.c[]
-------

难道这就是解决大量连接的终极方案
？  那我现在还写个大西瓜
？  不是。  首先，频繁创建销毁进程（线程）
在某些平台相当消耗资源。  在实际生产中，
你可能更倾向于使用线程池来节省创建新线程的开销。
但是有个根本上的问题，线程没法无限制的增长。  如果
您的程序需要在同一时刻处理成千上万的
连接，那可就和
在一个CPU处理几个线程完全不同了。

那么多线程也搞不定大量连接的问题，咋整？
In the Unix paradigm, you make your sockets _nonblocking_.  Unix
调用这样做：
[code,C]
------
fcntl(fd, F_SETFL, O_NONBLOCK);
------
fd是文件socket的文件描述符。  脚注：[文件描述符
是内核分配给已打开的文件数字编号。 你可以使用
this number to make Unix calls referring to the socket.]  Once you've
made fd (the socket) nonblocking, from then on, whenever you make a
network call to fd the call will either complete the operation
immediately or return with a special error code to indicate "I
couldn't make any progress now, try again."  So our two-socket example
might be naively written as:

//BUILD: FUNCTIONBODY INC:../example_stubs/sec01.h
.Bad Example: busy-polling all sockets
[code,C]
------
/* This will work, but the performance will be unforgivably bad. */
int i, n;
char buf[1024];
for (i=0; i < n_sockets; ++i)
    fcntl(fd[i], F_SETFL, O_NONBLOCK);

while (i_still_want_to_read()) {
    for (i=0; i < n_sockets; ++i) {
        n = recv(fd[i], buf, sizeof(buf), 0);
        if (n == 0) {
            handle_close(fd[i]);
        } else if (n < 0) {
            if (errno == EAGAIN)
                 ; /* The kernel didn't have any data for us to read. */
            else
                 handle_error(fd[i], errno);
         } else {
            handle_input(fd[i], buf, n);
         }
    }
}
------

Now that we're using nonblocking sockets, the code above would
_work_... but only barely.  The performance will be awful, for two
reasons.  First, when there is no data to read on either connection
the loop will spin indefinitely, using up all your CPU cycles.
Second, if you try to handle more than one or two connections with
this approach you'll do a kernel call for each one, whether it has
any data for you or not.  So what we need is a way to tell the kernel
"wait until one of these sockets is ready to give me some data, and
tell me which ones are ready."

The oldest solution that people still use for this problem is
select().  The select() call takes three sets of fds (implemented as
bit arrays): one for reading, one for writing, and one for
"exceptions".  It waits until a socket from one of the sets is ready
and alters the sets to contain only the sockets ready for use.

Here is our example again, using select:

//BUILD: FUNCTIONBODY INC:../example_stubs/sec01.h
.Example: Using select
[code,C]
------
/* If you only have a couple dozen fds, this version won't be awful */
fd_set readset;
int i, n;
char buf[1024];

while (i_still_want_to_read()) {
    int maxfd = -1;
    FD_ZERO(&readset);

    /* Add all of the interesting fds to readset */
    for (i=0; i < n_sockets; ++i) {
         if (fd[i]>maxfd) maxfd = fd[i];
         FD_SET(fd[i], &readset);
    }

    /* Wait until one or more fds are ready to read */
    select(maxfd+1, &readset, NULL, NULL, NULL);

    /* Process all of the fds that are still set in readset */
    for (i=0; i < n_sockets; ++i) {
        if (FD_ISSET(fd[i], &readset)) {
            n = recv(fd[i], buf, sizeof(buf), 0);
            if (n == 0) {
                handle_close(fd[i]);
            } else if (n < 0) {
                if (errno == EAGAIN)
                     ; /* The kernel didn't have any data for us to read. */
                else
                     handle_error(fd[i], errno);
             } else {
                handle_input(fd[i], buf, n);
             }
        }
    }
}
------


And here's a reimplementation of our ROT13 server, using select() this
time.

//BUILD: SKIP
.Example: select()-based ROT13 server
[code,C]
------
include::examples_01/01_rot13_server_select.c[]
------

But we're still not done.  Because generating and reading the select()
bit arrays takes time proportional to the largest fd that you provided
for select(), the select() call scales terribly when the number of
sockets is high.  footnote:[On the userspace side, generating and
reading the bit arrays can be made to take time proportional to the
number of fds that you provided for select().  But on the kernel side,
reading the bit arrays takes time proportional to the largest fd in the
bit array, which tends to be around _the total number of fds in use in
the whole program_, regardless of how many fds are added to the sets in
select().]

Different operating systems have provided different replacement
functions for select.  These include poll(), epoll(), kqueue(),
evports, and /dev/poll.  All of these give better performance than
select(), and all but poll() give O(1) performance for adding a socket,
removing a socket, and for noticing
that a socket is ready for IO.

Unfortunately, none of the efficient interfaces is a ubiquitous
standard.  Linux has epoll(), the BSDs (including Darwin) have
kqueue(), Solaris has evports and /dev/poll... _and none of these
operating systems has any of the others_.  So if you want to write a
portable high-performance asynchronous application, you'll need an
abstraction that wraps all of these interfaces, and provides whichever
one of them is the most efficient.

And that's what the lowest level of the Libevent API does for you.  It
provides a consistent interface to various select() replacements,
using the most efficient version available on the computer where it's
running.

Here's yet another version of our asynchronous ROT13 server.  This
time, it uses Libevent 2 instead of select().  Note that the fd_sets
are gone now: instead, we associate and disassociate events with a
struct event_base, which might be implemented in terms of select(),
poll(), epoll(), kqueue(), etc.

//BUILD: SKIP
.Example: A low-level ROT13 server with Libevent
[code,C]
-------
include::examples_01/01_rot13_server_libevent.c[]
-------

(Other things to note in the code: instead of typing the sockets as
"int", we're using the type evutil_socket_t.  Instead of calling
fcntl(O_NONBLOCK) to make the sockets nonblocking, we're calling
evutil_make_socket_nonblocking.  These changes make our code compatible
with the divergent parts of the Win32 networking API.)


What about convenience?  (and what about Windows?)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You've probably noticed that as our code has gotten more efficient,
it has also gotten more complex.  Back when we were forking, we didn't
have to manage a buffer for each connection: we just had a separate
stack-allocated buffer for each process.  We didn't need to explicitly
track whether each socket was reading or writing: that was implicit in
our location in the code.  And we didn't need a structure to track how
much of each operation had completed: we just used loops and stack
variables.

Moreover, if you're deeply experienced with networking on Windows,
you'll realize that Libevent probably isn't getting optimal
performance when it's used as in the example above.  On Windows, the
way you do fast asynchronous IO is not with a select()-like interface:
it's by using the IOCP (IO Completion Ports) API.  Unlike all the
fast networking APIs, IOCP does not alert your program when a socket
is _ready_ for an operation that your program then has to perform.
Instead, the program tells the Windows networking stack to _start_ a
network operation, and IOCP tells the program when the operation has
finished.

Fortunately, the Libevent 2 "bufferevents" interface solves both of
these issues: it makes programs much simpler to write, and provides
an interface that Libevent can implement efficiently on Windows _and_
on Unix.

Here's our ROT13 server one last time, using the bufferevents API.

//BUILD: SKIP
.Example: A simpler ROT13 server with Libevent
[code,C]
-------
include::examples_01/01_rot13_server_bufferevent.c[]
-------

How efficient is all of this, really?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

XXXX write an efficiency section here.  The benchmarks on the libevent
page are really out of date.


