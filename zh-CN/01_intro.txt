include::license.txt[]

:language: C

对于异步IO 的简单介绍
----------------------------------------

大多数初阶程序员最先接触的都是阻塞IO调用。
当你进行IO调用时，
系统并不会立刻返回直到它的业务逻辑处理完毕
，或者直到网络超时，那么，这个IO请求就是同步的。  当你在一个TCP连接上调用"connect()"
，相当于，你的操作系统以队列的方式发送了一个SYN包至
TCP连接的另一端主机。  在收到对方主机回应的SYN ACK包之前，它并不会
把控制权交还给你的应用，
亦或是系统超时
导致放弃挣扎。

这里有一个简单的阻塞式网络客户端的
调用示例。  它连接至www.google.com，发送一个简单的
HTTP请求，并且将响应结果输出值stdout。

//BUILD: SKIP
.Example: A simple blocking HTTP client
[code,C]
-------
include::examples_01/01_sync_webclient.c[]
-------

以上的所有网络请求都是阻塞式的：
gethostbyname只有在查询到www.google.com成功或者失败解析后
才会真正返回；connect不会return除非真正的
建立了连接；recv调用不会返回除非接收到了数据
或者被服务端关闭；send请求不会返回除非
kernel的写缓冲区被flush。

当然，阻塞式IO也并非完全是垃圾。  如果不需要
你的程序在处理网络请求间歇做一些别的工作，阻塞式的IO不会出啥篓子
。  但是一旦你希望程序同时
处理多个连接。  比如这个场景：
假设你需要从两个连接中读取数据，并且你不确定
哪一个连接会先得到输入。  你可不能这样

//BUILD: FUNCTIONBODY INC:../example_stubs/sec01.h
.Bad Example
[code,C]
-------
/* This won't work. */
char buf[1024];
int i, n;
while (i_still_want_to_read()) {
    for (i=0; i<n_sockets; ++i) {
        n = recv(fd[i], buf, sizeof(buf), 0);
        if (n==0)
            handle_close(fd[i]);
        else if (n<0)
            handle_error(fd[i], errno);
        else
            handle_input(fd[i], buf, n);
    }
}
-------
因为一旦fd[2]的数据先来，你的程序就emo了
它在读到fd[0]和fd[1]的数据之前动都不会动一下
。

使用多线程
或者多进程服务器是一种解决方案。  并行处理的一种简单方案是
使用一个单独的进程(或线程)来处理每个连接。
一旦每个连接有一个单独的进程进行处理，一个进程内的阻塞式IO调用
就不会影响到其他导致block。

以下是另一个程序示例。  演示了一个小型的服务器，
监听40713端口的TCP连接，从建立连接的输入中
一次读取一行数据
。  通过Unix 系统调用fork() 来创建一个新的进程来
处理所有请求建立的连接。

//BUILD: SKIP
.Example: Forking ROT13 server
[code,C]
-------
include::examples_01/01_rot13_server_forking.c[]
-------

难道这就是解决大量连接的终极方案
？  那我现在还写个大西瓜
？  不是。  首先，频繁创建销毁进程（线程）
在某些平台相当消耗资源。  在实际生产中，
你可能更倾向于使用线程池来节省创建新线程的开销。
但是有个根本上的问题，线程没法无限制的增长。  如果
您的程序需要在同一时刻处理成千上万的
连接，那可就和
在一个CPU处理几个线程完全不同了。

那么多线程也搞不定大量连接的问题，咋整？
在Unix中，可以通过这样让  Unix
的调用非阻塞：
[code,C]
------
fcntl(fd, F_SETFL, O_NONBLOCK);
------
fd是文件socket的文件描述符。  脚注：[文件描述符
是内核分配给已打开的文件数字编号。 你可以使用
这个数字编号来对指向的socket套接字进行系统调用。] 一旦
socket fd描述符被设置为nonblocking，对他进行的
任何网络调用都会
立刻响应或者返回一个特殊的错误码，
表示“现在不行，重试一下。” 所以，示例
可以原生重写成这样：

//BUILD: FUNCTIONBODY INC:../example_stubs/sec01.h
.Bad Example: busy-polling all sockets
[code,C]
------
/* 这样ok，但是性能垃圾。 */
int i, n;
char buf[1024];
for (i=0; i < n_sockets; ++i)
    fcntl(fd[i], F_SETFL, O_NONBLOCK);

while (i_still_want_to_read()) {
    for (i=0; i < n_sockets; ++i) {
        n = recv(fd[i], buf, sizeof(buf), 0);
        if (n == 0) {
            handle_close(fd[i]);
        } else if (n < 0) {
            if (errno == EAGAIN)
                 ; /* The kernel didn't have any data for us to read. */
            else
                 handle_error(fd[i], errno);
         } else {
            handle_input(fd[i], buf, n);
         }
    }
}
------

现在已经使用了非阻塞的socket，以上代码
应该工作， 但是相当粗暴。  性能将会很“爆炸”，
因为两个原因。  第一，当系统中没有任何一个连接有数据可以读取
的时候，就会进入死循环，把你的CPU跑满。
第二，如果用这个方法处理多个连接
不管有没有数据，
都会进行系统调用。  所以我们需要一种方法告诉内核
“等到这些socket连接准备好发送数据，
并且告诉我哪几个准备好了。“

最古老的解决此问题的方式是
select()。  select() 调用需要三套fds（
bit数组实现）：一个用于读取，一个用于写入，一个用于
“异常”。  他会阻塞知道其中的一些套接字集合准备就绪
并且调整集合提供可供使用的套接字。

接下来的例子，使用了select：

//BUILD: FUNCTIONBODY INC:../example_stubs/sec01.h
.Example: Using select
[code,C]
------
/* If you only have a couple dozen fds, this version won't be awful */
fd_set readset;
int i, n;
char buf[1024];

while (i_still_want_to_read()) {
    int maxfd = -1;
    FD_ZERO(&readset);

    /* Add all of the interesting fds to readset */
    for (i=0; i < n_sockets; ++i) {
         if (fd[i]>maxfd) maxfd = fd[i];
         FD_SET(fd[i], &readset);
    }

    /* Wait until one or more fds are ready to read */
    select(maxfd+1, &readset, NULL, NULL, NULL);

    /* Process all of the fds that are still set in readset */
    for (i=0; i < n_sockets; ++i) {
        if (FD_ISSET(fd[i], &readset)) {
            n = recv(fd[i], buf, sizeof(buf), 0);
            if (n == 0) {
                handle_close(fd[i]);
            } else if (n < 0) {
                if (errno == EAGAIN)
                     ; /* The kernel didn't have any data for us to read. */
                else
                     handle_error(fd[i], errno);
             } else {
                handle_input(fd[i], buf, n);
             }
        }
    }
}
------


下面是一个ROT13 server的实现，使用的是select()
。

//BUILD: SKIP
.Example: select()-based ROT13 server
[code,C]
------
include::examples_01/01_rot13_server_select.c[]
------

但还没完。  因为生成和读取select()
的bit数组中需要的时间和你提供的最大连接fd成正比
，在
socket连接数很高的情况下，select() 调用进行扩容是一件可怕的事儿。  脚注：[在用户侧，生成和读取
bit数组的时间开销与你提供给
select() 的fds数量成正比。  但是在kenerl侧，
读取bit数组的时间与最大的fd有关
，这往往就是程序中fd的总数，
不管多少fd被加入到
select()调用中。]

不同的操作系统提供了不同的
select调用方法。  其中就有poll(), epoll(), kqueue(),
evport和/dev/poll。  除了poll()， 所有这些的性能都比
select() 好，增加、
删除、通知socket ready的性能
都是O(1)。

可惜，这些接口都没有所有平台的
标准实现。  Linux有epoll(), BSDs（包括Darwin）有
kqueue(), Solaris 有evport和 /dev/poll... 这些
操作系统都没有其他的实现。  所以如果你想写一个
方便移植的高性能异步应用，你必须要有一个
以上所有系统接口的抽象实现，可以高效的提供
其中一种接口实现。

这就是Libevent API提供给你的底层能力。  它
提供了各种各样select() 实现的一致性接口，
挑选并且使用运行计算机上的最优版本
。

以下是另一个版本ROT13服务的实现。  这
次，我们使用Libevent2 代替select()。  注意fd_sets
现在已经没有了：取而代之的是，我们通过
event_base结构体将事件关联与取消关联，也就是select()，
poll()， epoll()，kqueue()，等等的实现。

//BUILD: SKIP
.Example: A low-level ROT13 server with Libevent
[code,C]
-------
include::examples_01/01_rot13_server_libevent.c[]
-------

（代码中另一些说明：sockets不再是"int"，
我们使用evutil_socket_t这个类型。  我们使用evutil_make_socket_nonblocking来
取代fcntl(O_NONBLOCK)以设置sockets非阻塞
。  通过这些更改，我们的代码
与 Win32 网络 API的不同部分能够兼容。)


够不够方便？  （包括Windows下？）
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

你已经注意到随着我们的代码变得更高效，
它也越来越复杂。  forking的时候，并不需要
管理每个connnection的buffer：我们只需要
为每个进程申请单独的堆buffer空间。  我们不需要明确
追踪是不是每个套接字正在读取或者写入：这些都隐含
在我们的代码中。  并且我们也不需要一个结构来追踪有
多少的操作已经完成：只需要循环栈内变量
。

此外，如果您在Windows上具有很强的网络经验，
你会意识到Libevent 可能不能如
以往示例中展示的那么高的性能。  在 Windows 上
异步IO的接口与select()不类似：
it's by using the IOCP (IO Completion Ports) API.  Unlike all the
fast networking APIs, IOCP does not alert your program when a socket
is _ready_ for an operation that your program then has to perform.
Instead, the program tells the Windows networking stack to _start_ a
network operation, and IOCP tells the program when the operation has
finished.

Fortunately, the Libevent 2 "bufferevents" interface solves both of
these issues: it makes programs much simpler to write, and provides
an interface that Libevent can implement efficiently on Windows _and_
on Unix.

Here's our ROT13 server one last time, using the bufferevents API.

//BUILD: SKIP
.Example: A simpler ROT13 server with Libevent
[code,C]
-------
include::examples_01/01_rot13_server_bufferevent.c[]
-------

How efficient is all of this, really?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

XXXXX 这里留着放效率说明。  libevent的benchmark
已经过期。


